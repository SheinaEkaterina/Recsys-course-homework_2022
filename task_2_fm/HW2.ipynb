{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Sequence, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "RAW = DATA_DIR / \"raw\"\n",
    "INTERIM = DATA_DIR / \"interim\"\n",
    "PROCESSED = DATA_DIR / \"processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом шаге\n",
    "* удалим столбцы, которые не используются в данном задании;\n",
    "* разделим датасет, выделив данные от самой поздней даты -- 2 октября 2021 года -- в тестовую выборку.\n",
    "\n",
    "В первом ДЗ у меня возникали проблемы с нехваткой оперативной памяти, поэтому в данном задании я существенно изменил реализацию. Например, при удалении столбцов не используется `pandas`, исходный файл считывается построчно. Естественно, из-за подобных изменений увеличились время выполнения операций и объём используемой памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(file: Union[Path, str],\n",
    "              file_out: Union[Path, str],\n",
    "              cols: Sequence[str] = []):\n",
    "    \"Copy csv file without selected columns\"\n",
    "    # helper function for applying mask to a csv file line\n",
    "    def filter_line(line: str, mask: list):\n",
    "        line_lst = line.rstrip().split(\",\")\n",
    "        filtered = [col for col, drop in zip(line_lst, mask) if not drop]\n",
    "        return \",\".join(filtered) + \"\\n\"\n",
    "\n",
    "    with open(file, \"r\") as fin:\n",
    "        # get boolean mask from header\n",
    "        header = next(fin)\n",
    "        header_lst = header.rstrip().split(',')\n",
    "        mask = [col in cols for col in header_lst]\n",
    "\n",
    "        # apply mask to every line and export results\n",
    "        with open(file_out, \"w\") as fout:\n",
    "            fout.write(filter_line(header, mask))\n",
    "            for line in fin:\n",
    "                fout.write(filter_line(line, mask))\n",
    "\n",
    "\n",
    "def str_to_date(s: str) -> datetime.date:\n",
    "    return datetime.fromisoformat(s).date()\n",
    "\n",
    "\n",
    "def date_split(file: Union[Path, str],\n",
    "               output_dir: Union[Path, str],\n",
    "               test_date: str = \"2021-10-02\",\n",
    "               date_col: int = 0):\n",
    "    \"\"\"\n",
    "    Split csv file by putting all entries with `test_date` date into test\n",
    "    dataset. Creates `train.csv` and `test.csv` in `output_dir`.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    assert output_dir.exists()\n",
    "    test_date = str_to_date(test_date)\n",
    "\n",
    "    with open(file, \"r\") as infile:\n",
    "        # output file objects\n",
    "        f_train = open(output_dir / \"train.csv\", \"w\")\n",
    "        f_test = open(output_dir / \"test.csv\", \"w\")\n",
    "\n",
    "        # write header\n",
    "        header = next(infile)\n",
    "        f_train.write(header)\n",
    "        f_test.write(header)\n",
    "\n",
    "        # copy data to one of the files line by line\n",
    "        for line in infile:\n",
    "            date_str = line.split(\",\")[date_col]\n",
    "            date = str_to_date(date_str)\n",
    "            fout = f_test if date == test_date else f_train\n",
    "            fout.write(line)\n",
    "\n",
    "        f_train.close()\n",
    "        f_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time,zone_id,banner_id,oaid_hash,campaign_clicks,os_id,country_id,impressions,clicks\r\n",
      "2021-09-27 00:01:30.000000,0,0,5664530014561852622,0,0,0,1,1\r\n",
      "2021-09-26 22:54:49.000000,1,1,5186611064559013950,0,0,1,1,1\r\n",
      "2021-09-26 23:57:20.000000,2,2,2215519569292448030,3,0,0,1,1\r\n",
      "2021-09-27 00:04:30.000000,3,3,6262169206735077204,0,1,1,1,1\r\n",
      "2021-09-27 00:06:21.000000,4,4,4778985830203613115,0,1,0,1,1\r\n",
      "2021-09-27 00:06:50.000000,5,5,2377014068362699676,0,2,2,1,1\r\n",
      "2021-09-27 00:07:34.000000,6,6,6863358899511896876,0,3,0,1,1\r\n",
      "2021-09-27 00:08:49.000000,7,7,2876502170484631685,0,4,1,1,1\r\n",
      "2021-09-27 00:09:08.000000,8,8,5839858970958967275,0,4,3,1,1\r\n"
     ]
    }
   ],
   "source": [
    "drop_cols(file=RAW / \"data.csv\",\n",
    "          file_out=INTERIM / \"data.csv\",\n",
    "          cols=[\"banner_id0\", \"banner_id1\", \"rate0\", \"rate1\", \"g0\", \"g1\",\n",
    "                \"coeff_sum0\", \"coeff_sum1\"])\n",
    "date_split(file=INTERIM / \"data.csv\",\n",
    "           output_dir=INTERIM,\n",
    "           test_date=\"2021-10-02\",\n",
    "           date_col=0)\n",
    "\n",
    "!head {INTERIM / \"train.csv\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь выполним такие же преобразования, как и в ДЗ №1. При этом немного поменяем способ хранения категориальных данных: вместо разреженных one-hot массивов будем использовать токены от $0$ до $n$. Как и в прошлом задании будем ограничивать число уникальных токенов (one-hot признаков), объединяя редкие значения.\n",
    "\n",
    "Изменение вида представления данных вызвано тем, что для реализации FM в этом задании использовался пакет *PyTorch*, в котором разреженные массивы пока поддерживают только ограниченное число операций. Поэтому аналогичная показанной ниже реализация FM, использующая разреженные данные, оказываетcя существенно медленнее (по крайней мере, у меня получилось так)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_KWARGS = {\n",
    "    \"zone_id\": dict(min_frequency=100),\n",
    "    \"banner_id\": dict(min_frequency=20),\n",
    "    \"oaid_hash\": dict(min_frequency=20),\n",
    "    \"os_id\": dict(max_categories=8),\n",
    "    \"country_id\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(data: pd.Series,\n",
    "                  min_frequency: Optional[int] = None,\n",
    "                  max_categories: Optional[int] = None\n",
    "                  ) -> defaultdict:\n",
    "    \"\"\"\n",
    "    Return a mapping from categorical features to integers from 0 to n.\n",
    "    Order is defined by frequencies, n can be limited by function arguments.\n",
    "    \"\"\"\n",
    "    vc = data.value_counts(sort=True)  # series: value -> count\n",
    "    if min_frequency is not None:\n",
    "        if 0 < min_frequency < 1:\n",
    "            min_frequency *= len(data)\n",
    "        vc = vc[vc >= min_frequency]\n",
    "    if max_categories is not None:\n",
    "        vc = vc.iloc[:max_categories - 1]  # + default value\n",
    "    n = len(vc)\n",
    "    return defaultdict(lambda: n, zip(vc.index, range(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_dense(input_dir: Union[Path, str],\n",
    "                              output_dir: Union[Path, str]):\n",
    "    output_dir = Path(output_dir)\n",
    "    assert output_dir.exists()\n",
    "\n",
    "    print(\"Reading data...\")\n",
    "    df_train = pd.read_csv(Path(input_dir) / \"train.csv\")\n",
    "    df_test = pd.read_csv(Path(input_dir) / \"test.csv\")\n",
    "    dataframes = (df_train, df_test)\n",
    " \n",
    "    # numerical features\n",
    "    print(\"Transforming numerical features...\")\n",
    "    for df in dataframes:\n",
    "        dt = pd.to_datetime(df[\"date_time\"])\n",
    "        df[\"weekday\"] = dt.dt.weekday\n",
    "        df[\"hour\"] = dt.dt.hour\n",
    "        df[\"log_campaign_clicks\"] = np.log(df[\"campaign_clicks\"] + 1)\n",
    "        df.drop(columns=[\"date_time\", \"campaign_clicks\", \"impressions\"],\n",
    "                inplace=True)\n",
    "\n",
    "    # categorical features\n",
    "    print(\"Transforming categorical features...\")\n",
    "    for name, kwargs in ONE_HOT_KWARGS.items():\n",
    "        tok = get_tokenizer(df_train[name], **kwargs)\n",
    "        for df in dataframes:\n",
    "            unchanged = np.ones(len(df), dtype=np.bool8)\n",
    "            for value, token in tqdm(tok.items()):\n",
    "                mask = (df[name] == value) & unchanged\n",
    "                df.loc[mask, name] = token\n",
    "                unchanged[mask] = False\n",
    "            df.loc[unchanged, name] = tok.default_factory()\n",
    "\n",
    "    # export results\n",
    "    df_train.to_csv(output_dir / \"train.csv\", index=False)\n",
    "    df_test.to_csv(output_dir / \"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Transforming numerical features...\n",
      "Transforming categorical features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 793/793 [00:19<00:00, 41.55it/s]\n",
      "100%|████████████████████████████████████████| 793/793 [00:03<00:00, 226.34it/s]\n",
      "100%|███████████████████████████████████████| 1208/1208 [00:27<00:00, 44.30it/s]\n",
      "100%|██████████████████████████████████████| 1208/1208 [00:04<00:00, 277.30it/s]\n",
      "100%|█████████████████████████████████████| 58846/58846 [21:38<00:00, 45.32it/s]\n",
      "100%|████████████████████████████████████| 58846/58846 [03:12<00:00, 305.72it/s]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00,  8.81it/s]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 52.95it/s]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:01<00:00, 14.53it/s]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 87.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_id,banner_id,oaid_hash,os_id,country_id,clicks,weekday,hour,log_campaign_clicks\r\n",
      "4,14,58846,1,0,1,0,0,0.0\r\n",
      "5,291,58846,1,4,1,6,22,0.0\r\n",
      "9,8,4525,1,0,1,6,23,1.3862943611198906\r\n",
      "19,2,58846,2,4,1,0,0,0.0\r\n",
      "206,159,58846,2,0,1,0,0,0.0\r\n",
      "115,503,58846,0,16,1,0,0,0.0\r\n",
      "85,20,58846,4,0,1,0,0,0.0\r\n",
      "94,22,58846,3,4,1,0,0,0.0\r\n",
      "18,88,58846,3,5,1,0,0,0.0\r\n"
     ]
    }
   ],
   "source": [
    "feature_engineering_dense(INTERIM, PROCESSED)\n",
    "\n",
    "!head {PROCESSED / \"train.csv\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для работы с преобразованными данными. Категориальные и количественные признаки возвращаются как два отдельных массива (`Xc` и `Xn`, соответственно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickDatasetTokenized(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that uses categorical data encoded as tokens {0, 1, .., n}.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        file: Union[Path, str],\n",
    "        categorical: Tuple[str] = \\\n",
    "            (\"oaid_hash\", \"banner_id\", \"country_id\", \"os_id\", \"zone_id\"),\n",
    "        numerical: Tuple[str] = \\\n",
    "            (\"weekday\", \"hour\", \"log_campaign_clicks\"),\n",
    "        target: str = \"clicks\"\n",
    "    ):\n",
    "        # load dataframe\n",
    "        df = pd.read_csv(file)\n",
    "        # categorical features\n",
    "        self.Xc = np.zeros(shape=(len(df), len(categorical)), dtype=np.int32)\n",
    "        self.Xc[:, :] = df.loc[:, categorical]\n",
    "        self.cat_sizes = tuple([len(df[col].unique()) for col in categorical])\n",
    "        # numerical features\n",
    "        self.Xn = np.zeros(shape=(len(df), len(numerical)), dtype=np.float32)\n",
    "        self.Xn[:, :] = df.loc[:, numerical]\n",
    "        # target\n",
    "        self.y = df[target].to_numpy().astype(np.float32)\n",
    "\n",
    "        # save feature names\n",
    "        self.categorical = categorical\n",
    "        self.numerical = numerical\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[np.ndarray]:\n",
    "        return self.Xc[index], self.Xn[index], self.y[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ClickDatasetTokenized(PROCESSED / \"train.csv\")\n",
    "ds_test = ClickDatasetTokenized(PROCESSED / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказания в FM делаются по формуле\n",
    "\\begin{equation*}\n",
    "    y_s =\n",
    "        \\theta + \\sum_{i=1}^{n}\\theta_i x_{is}\n",
    "        +\\sum_{i=1}^{n-1}\\sum_{j=i+1}^n\n",
    "            \\langle \\mathbf{w}_i, \\mathbf{w}_j \\rangle x_{is} x_{js}.\n",
    "\\end{equation*}\n",
    "\n",
    "Следуя [соответствующей главе Dive into Deep Learning](https://d2l.ai/chapter_recommender-systems/fm.html) преобразуем последнее слагаемое\n",
    "\\begin{equation*}\n",
    "    \\sum_{i=1}^{n-1}\\sum_{j=i+1}^n\n",
    "            \\langle \\mathbf{w}_i, \\mathbf{w}_j \\rangle x_{is} x_{js}\n",
    "    =\n",
    "    \\frac{1}{2} \\sum_{k=1}^d \\left(\n",
    "        \\left(\\sum_{i=1}^n{\\mathbf{w}_{i,k} x_{is, k}} \\right)^2\n",
    "         - \\sum_{i=1}^d{\\mathbf{w}_{i,k}^2 x_{is,k}^2}\n",
    "    \\right).\n",
    "\\end{equation*}\n",
    "Здесь $d$ -- размерность эмбеддингов признаков.\n",
    "\n",
    "В линейной части модели для категориальных признаков будем вместо весов использовать эмбеддинги размерности $1$. Также для удобства пронумеруем значения всех категориальных признаков. Такое изменение позволяет использовать один модуль `nn.Embedding` для эмбеддингов всех признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineTokenized(nn.Module):\n",
    "    def __init__(self, categorical_sizes: Sequence[int],\n",
    "                 n_numerical: int, emb_dim: int = 10):\n",
    "        super().__init__()\n",
    "        self.linear_c = CategoricalLinear(categorical_sizes)\n",
    "        self.linear_n = nn.Linear(n_numerical, 1)\n",
    "        self.emb_c = Embeddings(categorical_sizes, emb_dim)\n",
    "        self.emb_n = nn.Parameter(torch.randn((n_numerical, emb_dim)))\n",
    "\n",
    "    def forward(self, Xc: Tensor, Xn: Tensor) -> Tensor:\n",
    "        linear = self.linear_c(Xc) + self.linear_n(Xn)\n",
    "\n",
    "        Xn = Xn.unsqueeze(2)\n",
    "        vx_c = self.emb_c(Xc)  # one-hot => no multiplication\n",
    "        vx_n = Xn * self.emb_n\n",
    "        vx = torch.cat([vx_c, vx_n], 1)\n",
    "        sq_sum = vx.mean(1)**2\n",
    "        sum_sq = (vx**2).mean(1)\n",
    "\n",
    "        logits = linear + 0.5 * torch.sum(sq_sum - sum_sq, 1, keepdim=True)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, dl: DataLoader) -> np.ndarray:\n",
    "        predictions = []\n",
    "        for batch in tqdm(dl):\n",
    "            Xc, Xn = batch[:2]\n",
    "            output = torch.sigmoid(self.forward(Xc, Xn)).cpu().numpy()\n",
    "            predictions += list(output.ravel())\n",
    "        return np.array(predictions, dtype=np.float32)\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, emb_numbers: Sequence[int], emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.offsets = torch.tensor(np.cumsum(emb_numbers) - emb_numbers[0],\n",
    "                                    dtype=torch.int32)\n",
    "        self.emb = nn.Embedding(np.sum(emb_numbers), emb_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.offsets\n",
    "        return self.emb(x)\n",
    "\n",
    "\n",
    "class CategoricalLinear(Embeddings):\n",
    "    def __init__(self, categorical_sizes: Sequence[int]):\n",
    "        super().__init__(categorical_sizes, 1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        output = super().forward(x)\n",
    "        return output.mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее обучим модель на полной обучающей выборке. Перед этим был выполнен ряд экспериментов для подбора гиперпараметров. В частности, для выбора размерности эмбеддингов $d$ выполнялся скрипт `cross_validation.py` с $d = 5$, $10$ и $20$. Графики обучения при $d = 5$ приведены ниже, увеличение $d$ не привело к улучшению результатов. Введение регуляризации (`weight_decay`) также не приносило никаких дивидендов.\n",
    "\n",
    "![train_loss](images/train_loss.png)\n",
    "![val_loss](images/val_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineTokenized(\n",
       "  (linear_c): CategoricalLinear(\n",
       "    (emb): Embedding(60875, 1)\n",
       "  )\n",
       "  (linear_n): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (emb_c): Embeddings(\n",
       "    (emb): Embedding(60875, 5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FactorizationMachineTokenized(\n",
    "    ds_train.cat_sizes, len(ds_train.numerical), 5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "dl_train = DataLoader(ds_train, batch_size=2048, shuffle=True)\n",
    "dl_test = DataLoader(ds_test, batch_size=4096, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6686/6686 [02:28<00:00, 45.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss = 0.1754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6686/6686 [02:17<00:00, 48.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss = 0.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6686/6686 [02:16<00:00, 48.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss = 0.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    num_iter = 0\n",
    "    avg_loss = 0\n",
    "    for Xc, Xn, y in tqdm(dl_train):\n",
    "        logits = model(Xc, Xn)\n",
    "        loss = loss_fn(logits, y.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        num_iter += 1\n",
    "        avg_loss *= (num_iter - 1) / num_iter\n",
    "        avg_loss += loss.item() / num_iter\n",
    "    print(f\"Epoch {epoch}: train_loss = {avg_loss:.4f}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 520/520 [00:14<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1346, test ROC-AUC:  0.774\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(dl_test)\n",
    "test_loss = log_loss(ds_test.y, pred, eps=1e-7)\n",
    "test_auc = roc_auc_score(ds_test.y, pred)\n",
    "print(f\"Test loss: {test_loss:.4f}, test ROC-AUC: {test_auc: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сведём результаты в таблицу.\n",
    "\n",
    "| Model  | log_loss | roc_auc_score |\n",
    "| ------ | -------- | ------------- |\n",
    "|baseline| 0.1549   | 0.5           |\n",
    "| linear | 0.1350   | 0.770         |\n",
    "| FM     | 0.1346   | 0.774         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
