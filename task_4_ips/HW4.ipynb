{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0ynYGMpFvFDi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я работала в гугл колабе, при необходимости поменяйте две ячейки внизу на свой способ хранения данных."
      ],
      "metadata": {
        "id": "e-j2cFWlgEFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6Wuc7o4vK4v",
        "outputId": "e40c0728-1990-4731-fb57-990fa34c02e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Recsys_data1.csv'"
      ],
      "metadata": {
        "id": "MNSwEvMmvPKC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "ZNTgNcb8vSV2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В анализе удалим то, что нам не нужно по задаче и сделаем обычные проверки данных: содержат ли данные nan и как они распределены по дням.\n",
        "\n",
        "Также сделаем несколько специфических проверок - проверим, чтобы g0 и g1 были положительными и узнаем, есть ли 0 в impressions."
      ],
      "metadata": {
        "id": "dK8zfDVKfnJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis(data: pd.DataFrame):\n",
        "    data = data.drop(['campaign_clicks'], axis=1) #remove unnecessary cols (according to the task)\n",
        "    print(data.head)\n",
        "\n",
        "    print(\"Data contains null is \", data.isnull().values.any()) #check if contains null\n",
        "\n",
        "    print(len(df[df['impressions'] == 0])) #check if impressions contains 0\n",
        "    \n",
        "    #count how many data on each day we have\n",
        "    cur = data.copy()\n",
        "    cur['date_time'] = cur['date_time'].apply(lambda x: x[:10])\n",
        "    cur = cur.groupby(['date_time']).size()\n",
        "    print(cur)\n",
        "\n",
        "    print(\"How many g0 are less than zero: \", len(data[data['g0'] < 0]))\n",
        "    print(\"How many g1 are less than zero: \", len(data[data['g1'] < 0]))\n",
        "    return data"
      ],
      "metadata": {
        "id": "owBkRFEVvVUO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = analysis(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSODI2HbveF9",
        "outputId": "74df5427-f047-4b0d-ea46-fe1096dcdd31"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of                            date_time  zone_id  banner_id            oaid_hash  \\\n",
            "0         2021-09-27 00:01:30.000000        0          0  5664530014561852622   \n",
            "1         2021-09-26 22:54:49.000000        1          1  5186611064559013950   \n",
            "2         2021-09-26 23:57:20.000000        2          2  2215519569292448030   \n",
            "3         2021-09-27 00:04:30.000000        3          3  6262169206735077204   \n",
            "4         2021-09-27 00:06:21.000000        4          4  4778985830203613115   \n",
            "...                              ...      ...        ...                  ...   \n",
            "15821467  2021-10-02 15:51:35.000000      146        530  4329496688011613719   \n",
            "15821468  2021-09-27 22:03:14.000000       12         22   453968700792456599   \n",
            "15821469  2021-10-02 17:41:10.000000       12       1236  9112780675655118328   \n",
            "15821470  2021-09-29 00:39:32.000000      967         21  6968514095695555037   \n",
            "15821471  2021-09-28 07:00:18.000000       19        635  8754492963501134426   \n",
            "\n",
            "          os_id  country_id  banner_id0  rate0        g0  coeff_sum0  \\\n",
            "0             0           0        1240  0.067  0.035016   -7.268846   \n",
            "1             0           1           1  0.002  0.054298   -2.657477   \n",
            "2             0           0           2  0.014  0.014096   -3.824875   \n",
            "3             1           1           3  0.012  0.015232   -3.461357   \n",
            "4             1           0           4  0.019  0.051265   -4.009026   \n",
            "...         ...         ...         ...    ...       ...         ...   \n",
            "15821467      2           9         530  0.020  0.040037   -3.495224   \n",
            "15821468      1           6    11464229  0.010  0.000000    0.000000   \n",
            "15821469      2           0        1236  0.046  0.021433   -5.986478   \n",
            "15821470      0           0          21  0.014  0.083901   -3.025758   \n",
            "15821471      0           3         635  0.050  0.058530   -4.898957   \n",
            "\n",
            "          banner_id1  rate1        g1  coeff_sum1  impressions  clicks  \n",
            "0                  0  0.010  0.049516   -5.369901            1       1  \n",
            "1                269  0.004  0.031942   -4.449220            1       1  \n",
            "2                 21  0.014  0.014906   -3.939309            1       1  \n",
            "3                 99  0.006  0.050671   -3.418403            1       1  \n",
            "4           11464230  6.790  0.032005   -2.828797            1       1  \n",
            "...              ...    ...       ...         ...          ...     ...  \n",
            "15821467         481  0.020  0.039634   -3.493091            1       0  \n",
            "15821468          22  0.004  0.008525   -5.630203            1       0  \n",
            "15821469        1234  0.046  0.021363   -5.986109            1       0  \n",
            "15821470           2  0.014  0.083865   -3.063858            1       0  \n",
            "15821471         631  0.050  0.078144   -4.951505            1       0  \n",
            "\n",
            "[15821472 rows x 16 columns]>\n",
            "Data contains null is  True\n",
            "0\n",
            "date_time\n",
            "2021-09-01          1\n",
            "2021-09-26    3102610\n",
            "2021-09-27    2367303\n",
            "2021-09-28    2307355\n",
            "2021-09-29    2420588\n",
            "2021-09-30    1851189\n",
            "2021-10-01    1643448\n",
            "2021-10-02    2128978\n",
            "dtype: int64\n",
            "How many g0 are less than zero:  5\n",
            "How many g1 are less than zero:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы из анализа данных:\n",
        "1. Есть ненужная числовая зависимость в колонках zone_id, banner_id, os_id, country_id, значит в обработке будем использовать например OneHotEncoding.\n",
        "\n",
        "2. В данных есть null. У нас нет никаких указаний, что делать с такими данными, так что просто их удалим в обработке.\n",
        "\n",
        "3. Все impressions равны 1 (что логично), так что уберем этот столбец.\n",
        "\n",
        "4. Также есть день 2021-09-01, для которого есть всего одна запись и он далеко от остальных. Видимо он попал в данные случайно, его надо удалить.\n",
        "\n",
        "5. Последний день (который будем брать в тест) 2021-10-02.\n",
        "\n",
        "6. Есть g0 и g1, которые меньше 0. Непонятно, как они оказались в данных, так что тоже удалим их как вбросы.\n",
        "\n",
        "7. Также по заданию нужно оставить только те строки, в которых banner_id = banner_id0\n",
        "\n",
        "Ровно этим и будем заниматься в функции feature_engineering. Правда в OneHotEncoding отправим все данные, потому что, они так сожмутся. А без сжатия трудно закодировать даже один столбец."
      ],
      "metadata": {
        "id": "xwvmMDyjv7oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#feature engineering according to the analysis and spliting into train and test\n",
        "def feature_engineering(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    #remove incorrect and unnecessary data\n",
        "    data = data.drop(['oaid_hash', 'rate0', 'rate1', 'impressions'], axis=1)\n",
        "    data = data[data['banner_id'] == data['banner_id0']]\n",
        "    data = data[data['date_time'] > '2021-09-20']\n",
        "    data = data.drop(['banner_id0'], axis=1)\n",
        "    data = data[(data['g1'] > 0) & (data['g0'] > 0)]\n",
        "    data = data.dropna()\n",
        "\n",
        "    #get coeffs of normal distribution for the last day\n",
        "    coeffs = data[['date_time', 'g0', 'g1', 'coeff_sum0',  'coeff_sum1']]\n",
        "    coeffs = coeffs[coeffs['date_time'] >= '2021-10-02'].drop(['date_time'], axis=1)\n",
        "\n",
        "    #fit encoder for linear classification task\n",
        "    data = data.drop(['g0', 'g1', 'coeff_sum0',  'coeff_sum1'], axis=1)\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc = enc.fit(data.drop(['clicks', 'date_time', 'banner_id1'], axis=1))\n",
        "\n",
        "    #get x_train and y_train for linear classification\n",
        "    cur = data[data['date_time'] < '2021-10-02']\n",
        "    x_train = cur.drop(['clicks', 'date_time', 'banner_id1'], axis=1)\n",
        "    x_train = enc.transform(x_train)\n",
        "    y_train = cur['clicks']\n",
        "\n",
        "    #get x_test0 with banner_id0 and y_test\n",
        "    cur = data[data['date_time'] >= '2021-10-02']\n",
        "    x_test0 = cur.drop(['clicks', 'date_time', 'banner_id1'], axis=1)\n",
        "    x_test0 = enc.transform(x_test0)\n",
        "    y_test = cur['clicks']\n",
        "\n",
        "    #get x_test1 with banner_id1\n",
        "    cur['banner_id'] = cur['banner_id1']\n",
        "    x_test1 = cur.drop(['clicks', 'date_time', 'banner_id1'], axis=1)\n",
        "    x_test1 = enc.transform(x_test1)\n",
        "    \n",
        "    return x_train, y_train, x_test0, x_test1, y_test, coeffs"
      ],
      "metadata": {
        "id": "Et2CHU42v87u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test0, x_test1, y_test, coeffs = feature_engineering(df)\n",
        "df = []"
      ],
      "metadata": {
        "id": "2rMJrLctKpqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Берем линейную модель из первой домашки. И на всякий случай считаем метрики."
      ],
      "metadata": {
        "id": "MOtBLvvTh24H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(solver='liblinear', C=0.01, penalty='l2', fit_intercept=False)\n",
        "model = model.fit(x_train, y_train)\n",
        "y_pred0 = model.predict(x_test0)\n",
        "y_pred1 = model.predict(x_test1)"
      ],
      "metadata": {
        "id": "FkPFRzy0O4BM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "auc_metric = roc_auc_score(y_test, model.predict_proba(x_test0)[:, 1])\n",
        "log_loss_metric = log_loss(y_test, model.predict_proba(x_test0))\n",
        "print('Auc: {:.3f}, log_loss: {:.3f}'.format(auc_metric, log_loss_metric))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6fW6K7kPtqq",
        "outputId": "0381dd00-151c-44a1-960d-1670a705ec06"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auc: 0.791, log_loss: 0.134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем coeff_sum0_new и coeff_sum1_new из предсказаний модели."
      ],
      "metadata": {
        "id": "47_dot9wiGMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import logit\n",
        "\n",
        "coeffs['coeff_sum0_new'] = logit(model.predict_proba(x_test0)[:, 1])\n",
        "coeffs['coeff_sum1_new'] = logit(model.predict_proba(x_test1)[:, 1])"
      ],
      "metadata": {
        "id": "ZgyrnmLAP5B_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем вероятность того, что одна нормальная величина больше, чем другая. Выкладки не привожу, но их можно найти например здесь:\n",
        "https://stats.stackexchange.com/questions/50501/probability-of-one-random-variable-being-greater-than-another"
      ],
      "metadata": {
        "id": "dK1Nw2PkiRxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "#sf = 1 - sdf\n",
        "coeffs['pi_0'] = norm.sf((coeffs['coeff_sum1'] - coeffs['coeff_sum0']) / np.sqrt(coeffs['g0'] ** 2 + coeffs['g1'] ** 2))\n",
        "coeffs['pi_1'] = norm.sf((coeffs['coeff_sum1_new'] - coeffs['coeff_sum0_new']) / np.sqrt(coeffs['g0'] ** 2 + coeffs['g1'] ** 2))"
      ],
      "metadata": {
        "id": "LdTdeuZ9YHse"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "И считаем cips для lambda=10"
      ],
      "metadata": {
        "id": "yPlkJn78ilCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lam = 10\n",
        "cips = np.sum(y_test * np.clip(coeffs['pi_1'] / coeffs['pi_0'], a_min=None, a_max=lam)) / len(y_test)\n"
      ],
      "metadata": {
        "id": "W8NxKmv3Yvpt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cips)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnE4HWHvZ0fP",
        "outputId": "9fc9aa48-3b98-490f-d6fa-a03f45a4f72d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07326100611835924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получили cips 0.07"
      ],
      "metadata": {
        "id": "S8ShtsiZitEM"
      }
    }
  ]
}